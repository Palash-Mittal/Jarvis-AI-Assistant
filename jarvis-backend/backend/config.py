LLM = {
    "default_model": "gemma2",      # or llama3 / phi3
    "ollama_executable": "ollama"   # path to ollama if not global
}

BACKEND = {
    "host": "127.0.0.1",
    "port": 8000
}
